{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General idea\n",
    "By ułatwić zrozumienie tematu spróbujemy każdy wątek tłumaczyć na 3 sposoby:\n",
    "1. Rookie level - jak dla dziecka\n",
    "2. Enthusiast level - na poziomie LO\n",
    "3. Math madness level - na poziomie studenta kierunków technicznych (matma, dużo i dużo pierdółkowatych pytań i odpowiedzi)\n",
    "\n",
    "\n",
    "# Machine Learning - overview\n",
    "## Rookie level\n",
    "\n",
    "Wykorzystanie sieci neuronowych do rozwiazywania różnych problemów optymalizacji.\n",
    "\n",
    "ML dzielimy ze względu na to jak \"uczymy\" (głównie) sieć neuronową tj.:\n",
    "1. Uczenie z nadzorem\n",
    "2. Uczenie bez nadzrou\n",
    "3. Uczenie ze wzmocnieniem\n",
    "+ bonusowo \"uczenie głębokie\" (deep learning)\n",
    "\n",
    "Jak rozwiązujemy problem z użyciem sieci neuronowej? \n",
    "1. Znajdujemy problem/klasę problemów (i dużo danych do niego).\n",
    "2. Znajdujemy jego możliwe rozwiązanie matematyczne (np. regresje liniową).\n",
    "3. Wybieramy jak będziemy uczyć sieć neuronową (normalnie eksperymentalnie ale pewnie ktoś już za nas to zrobił).\n",
    "4. Bazując na wybranej strategii uczenie sieci neuronowej, implementujemy ją (albo korzystamy z biblioteki jak sklearn lub Pytorch)\n",
    "5. Trenujemy, testujemy zaimplementowaną sieć neuronową.\n",
    "6. Cieszymy się z działającej (lub nie) sieci neuronowej rozwiazującej nasz problem!\n",
    "\n",
    "## Enthusiast level\n",
    "ML to połączenie informatyki i probabilistyki tj. zamiast pisać szereg warunków po prostu dajemy komputerowi dane a on ma rozpoznać wzorce w tych danych i podjąć odpowiednie działania."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ściągawka jak pisać wzory - do usunięcia\n",
    "$-\\frac{1}{m}{\\sum_{i=1}^n{\\sum_{j=1}^m y_{ij} ln(\\hat y_{ij})}}$\n",
    "\n",
    "In the previous example, we were guessing if in the house is or is not a dog. What if we want to guess if in the house is dog, cat, or pigeon? Cross-entropy we know from the previous example may not work, we need to use its variation, the multi-class cross-entropy!\n",
    "\n",
    "Let's see how our probabilities look like:\n",
    "* Red house:   $P_1(dog) = 0.7, P_2(cat) = 0.2, P_3(pigeon) = 0.1$\n",
    "* Blue house:  $P_1(dog) = 0.3, P_2(cat) = 0.4, P_3(pigeon) = 0.3$\n",
    "* Green house: $P_1(dog) = 0.1, P_2(cat) = 0.5, P_3(pigeon) = 0.4$\n",
    "\n",
    "Notice: each $P_i$ sum up for every animal need to give 1, example: $P_1(dog) + P_2(cat) + P_3(pigeon) = 0.7 + 0.2 + 0.1 = 1$. In other words sum of probabilities for each house need to be equal to 1.\n",
    "\n",
    "If we create the predicted output set like $y = (dog, pigeon, pigeon)$ we can sum it up:\n",
    "* predicted output $y = (dog, pigeon, pigeon)$\n",
    "* Probability product = $P_1(dog) x P_2(pigeon) x P_3(pigeon) = 0.7 x 0.3 x 0.4 = 0.084$\n",
    "* Cross-Entropy = $-ln(0.7) + -ln(0.3) + -ln(0.4) = 2.48$\n",
    "\n",
    "Ok, it starts to be a little bit messy. Let's clean it up and start by giving our houses numbers:\n",
    "* Red house will have number 1.\n",
    "* Blue house will have number 2.\n",
    "* Green house will have number 3.\n",
    "\n",
    "Next let's rename our probabilities:\n",
    "* House 1 (red):   $P_{11}(dog) = 0.7, P_{12}(cat) = 0.2, P_{13}(pigeon) = 0.1$\n",
    "* House 2 (blue):  $P_{21}(dog) = 0.3, P_{22}(cat) = 0.4, P_{23}(pigeon) = 0.3$\n",
    "* House 3 (green): $P_{31}(dog) = 0.1, P_{32}(cat) = 0.5, P_{33}(pigeon) = 0.4$\n",
    "\n",
    "Next step, clean expected output set:\n",
    "* $y_{1j}$ if dog is in house j (where j is between 1 and 3).\n",
    "* $y_{2j}$ if cat is in house j (where j is between 1 and 3).\n",
    "* $y_{3j}$ if pigeon is in house j (where j is between 1 and 3).\n",
    "\n",
    "Sum it up and we will get formula for multi-class cross-entropy:\n",
    "\n",
    "Multi-Class Cross-Entropy = $-{\\sum_{i=1}^n{\\sum_{j=1}^m y_{ij} ln(p_{ij})}}$\n",
    "\n",
    "Here *m* is number of classes so we want to get average from that so we will get:\n",
    "\n",
    "$-\\frac{1}{m}{\\sum_{i=1}^n{\\sum_{j=1}^m y_{ij} ln(p_{ij})}}$\n",
    "\n",
    "And of course our $p_{ij}$ is our predicted output $\\hat y_{ij}$ so we will get:\n",
    "\n",
    "$-\\frac{1}{m}{\\sum_{i=1}^n{\\sum_{j=1}^m y_{ij} ln(\\hat y_{ij})}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
