{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning - overview\n",
    "## Rooki level"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ściągawka jak pisać wzory - do usunięcia\n",
    "$-\\frac{1}{m}{\\sum_{i=1}^n{\\sum_{j=1}^m y_{ij} ln(\\hat y_{ij})}}$\n",
    "\n",
    "In the previous example, we were guessing if in the house is or is not a dog. What if we want to guess if in the house is dog, cat, or pigeon? Cross-entropy we know from the previous example may not work, we need to use its variation, the multi-class cross-entropy!\n",
    "\n",
    "Let's see how our probabilities look like:\n",
    "* Red house:   $P_1(dog) = 0.7, P_2(cat) = 0.2, P_3(pigeon) = 0.1$\n",
    "* Blue house:  $P_1(dog) = 0.3, P_2(cat) = 0.4, P_3(pigeon) = 0.3$\n",
    "* Green house: $P_1(dog) = 0.1, P_2(cat) = 0.5, P_3(pigeon) = 0.4$\n",
    "\n",
    "Notice: each $P_i$ sum up for every animal need to give 1, example: $P_1(dog) + P_2(cat) + P_3(pigeon) = 0.7 + 0.2 + 0.1 = 1$. In other words sum of probabilities for each house need to be equal to 1.\n",
    "\n",
    "If we create the predicted output set like $y = (dog, pigeon, pigeon)$ we can sum it up:\n",
    "* predicted output $y = (dog, pigeon, pigeon)$\n",
    "* Probability product = $P_1(dog) x P_2(pigeon) x P_3(pigeon) = 0.7 x 0.3 x 0.4 = 0.084$\n",
    "* Cross-Entropy = $-ln(0.7) + -ln(0.3) + -ln(0.4) = 2.48$\n",
    "\n",
    "Ok, it starts to be a little bit messy. Let's clean it up and start by giving our houses numbers:\n",
    "* Red house will have number 1.\n",
    "* Blue house will have number 2.\n",
    "* Green house will have number 3.\n",
    "\n",
    "Next let's rename our probabilities:\n",
    "* House 1 (red):   $P_{11}(dog) = 0.7, P_{12}(cat) = 0.2, P_{13}(pigeon) = 0.1$\n",
    "* House 2 (blue):  $P_{21}(dog) = 0.3, P_{22}(cat) = 0.4, P_{23}(pigeon) = 0.3$\n",
    "* House 3 (green): $P_{31}(dog) = 0.1, P_{32}(cat) = 0.5, P_{33}(pigeon) = 0.4$\n",
    "\n",
    "Next step, clean expected output set:\n",
    "* $y_{1j}$ if dog is in house j (where j is between 1 and 3).\n",
    "* $y_{2j}$ if cat is in house j (where j is between 1 and 3).\n",
    "* $y_{3j}$ if pigeon is in house j (where j is between 1 and 3).\n",
    "\n",
    "Sum it up and we will get formula for multi-class cross-entropy:\n",
    "\n",
    "Multi-Class Cross-Entropy = $-{\\sum_{i=1}^n{\\sum_{j=1}^m y_{ij} ln(p_{ij})}}$\n",
    "\n",
    "Here *m* is number of classes so we want to get average from that so we will get:\n",
    "\n",
    "$-\\frac{1}{m}{\\sum_{i=1}^n{\\sum_{j=1}^m y_{ij} ln(p_{ij})}}$\n",
    "\n",
    "And of course our $p_{ij}$ is our predicted output $\\hat y_{ij}$ so we will get:\n",
    "\n",
    "$-\\frac{1}{m}{\\sum_{i=1}^n{\\sum_{j=1}^m y_{ij} ln(\\hat y_{ij})}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
